{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "import numpy as np\n",
    "import scipy.io\n",
    "import matplotlib.pyplot as plt\n",
    "from test_utils import test\n",
    "from typing import NamedTuple, List, Dict, Tuple"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be working with the MovieLens dataset, containing 100k movie ratings.\n",
    "The goal is to predict the rating of a (movie, user) pair, given other ratings by this user and of this movie."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 100000 ratings of 943 users for 1682 movies.\n"
     ]
    }
   ],
   "source": [
    "class Dataset(NamedTuple):\n",
    "    \"\"\"Data container with three arrays of the same length:\"\"\"\n",
    "\n",
    "    movies: np.ndarray\n",
    "    users: np.ndarray\n",
    "    ratings: np.ndarray\n",
    "\n",
    "\n",
    "def load_data() -> Dataset:\n",
    "    \"\"\"Load a sparse matrix from a matlab file and return is as a list of\"\"\"\n",
    "    data = scipy.io.loadmat(\"movielens100k.mat\")[\"ratings\"]\n",
    "    movies, users = data.nonzero()  # indices of available ratings in the matrix\n",
    "    ratings = data[movies, users].A1\n",
    "    return Dataset(movies, users, ratings)\n",
    "\n",
    "\n",
    "dataset = load_data()\n",
    "\n",
    "num_users = np.max(dataset.users) + 1\n",
    "num_movies = np.max(dataset.movies) + 1\n",
    "\n",
    "print(\n",
    "    f\"Loaded {len(dataset.ratings)} ratings of {num_users} users for {num_movies} movies.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Plot the number of ratings per movie and user\n",
    "\n",
    "It will be too tricky to make predictions for movies and users for which too few ratings are available.\n",
    "Below we will investigate the distribution of how many ratings we have for various users and movies, to evaluate if we need to exclude some users or movies."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def count_ratings_per_movie(dataset: Dataset) -> Dict[int, int]:\n",
    "    \"\"\"Count the number of ratings available per movie\n",
    "\n",
    "    Inputs:\n",
    "        dataset: Dataset\n",
    "\n",
    "    Returns:\n",
    "        counts: a dictionary form movie id (int) -> count (int)\n",
    "\n",
    "    >>> count_ratings_per_movie(Dataset(np.array([0, 0, 1]), np.array([1, 2, 1]), np.array([1.0, 2.0, 3.0])))\n",
    "    {0: 2, 1: 1}\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    ####################################\n",
    "    ### ___ Enter your code here ___ ###\n",
    "    ####################################\n",
    "    return counts\n",
    "\n",
    "\n",
    "def count_ratings_per_user(dataset: Dataset) -> Dict[int, int]:\n",
    "    \"\"\"Count the number of ratings given by a user\n",
    "\n",
    "    Inputs:\n",
    "        dataset: Dataset\n",
    "\n",
    "    Returns:\n",
    "        counts: a dictionary form user id (int) -> count (int)\n",
    "\n",
    "    >>> count_ratings_per_user(Dataset(np.array([0, 0, 1]), np.array([1, 2, 2]), np.array([4.0, 1.0, 2.0])))\n",
    "    {1: 1, 2: 2}\n",
    "    \"\"\"\n",
    "    counts = {}\n",
    "    ####################################\n",
    "    ### ___ Enter your code here ___ ###\n",
    "    ####################################\n",
    "    return counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(count_ratings_per_movie)\n",
    "test(count_ratings_per_user)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_ratings_per_movie = count_ratings_per_movie(dataset)\n",
    "num_ratings_per_user = count_ratings_per_user(dataset)\n",
    "\n",
    "fig, axes = plt.subplots(ncols=2, figsize=(10, 2.5))\n",
    "for ax, ratings, unit in zip(\n",
    "    axes, (num_ratings_per_movie, num_ratings_per_user), (\"movie\", \"user\")\n",
    "):\n",
    "    ax.hist(ratings.values())\n",
    "    ax.set_title(f\"# Ratings per {unit} (histogram)\")\n",
    "    ax.set_xlabel(\"# Ratings\")\n",
    "    ax.set_ylabel(\"# Occurences\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\n",
    "    \"min # of movies per user = {}, \\nmin # of users per movie = {}.\".format(\n",
    "        min(num_ratings_per_user.values()), min(num_ratings_per_movie.values())\n",
    "    )\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Remove movies with too few ratings\n",
    "\n",
    "As is typical for real-life data, those distributions have a very long tail. There are some movies that have only one rating. Predictions for those will be very unreliable, so we want to take them out of the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_rare_movies(dataset: Dataset, min_ratings_per_movie=10) -> Dataset:\n",
    "    \"\"\"Filter a dataset, and keep only movies with a minimum number of reviews\n",
    "\n",
    "    Inputs:\n",
    "        dataset: Dataset\n",
    "        min_cases: int, the required minimum number\n",
    "\n",
    "    Returns:\n",
    "        new dataset: Dataset\n",
    "\n",
    "    >>> test_dataset = Dataset(np.array([0, 0, 1]), np.array([0, 1, 1]), np.array([1.0, 1.0, 1.0]))\n",
    "    >>> remove_rare_movies(test_dataset, 2)\n",
    "    Dataset(movies=array([0, 0]), users=array([0, 1]), ratings=array([1., 1.]))\n",
    "    \"\"\"\n",
    "    num_ratings_per_movie = count_ratings_per_movie(dataset)\n",
    "    ####################################\n",
    "    ### ___ Enter your code here ___ ###\n",
    "    ####################################\n",
    "    return pruned_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(remove_rare_movies)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pruned_dataset = remove_rare_movies(dataset)\n",
    "print(\n",
    "    f\"There are {len(pruned_dataset.ratings)} ratings left after pruning the dataset.\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Split the data into a train and test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(dataset, p_test=0.1, seed=1):\n",
    "    \"\"\"\n",
    "    Split a dataset randomly into a train and a test part\n",
    "\n",
    "    Inputs:\n",
    "        dataset: Dataset\n",
    "        p_test: float\n",
    "            propability (0 < p_test < 1) for a data point to go into the test set\n",
    "        seed: integer\n",
    "\n",
    "    Returns:\n",
    "        train_dataset: Dataset\n",
    "        test_dataset: Dataset\n",
    "\n",
    "    >>> split_dataset(Dataset(np.array([0, 0]), np.array([1, 0]), np.array([2.0, 1.0])), p_test=0)\n",
    "    (Dataset(movies=array([0, 0]), users=array([1, 0]), ratings=array([2., 1.])), Dataset(movies=array([], dtype=int64), users=array([], dtype=int64), ratings=array([], dtype=float64)))\n",
    "\n",
    "    >>> split_dataset(Dataset(np.array([0, 0]), np.array([1, 0]), np.array([2.0, 1.0])), p_test=1)\n",
    "    (Dataset(movies=array([], dtype=int64), users=array([], dtype=int64), ratings=array([], dtype=float64)), Dataset(movies=array([0, 0]), users=array([1, 0]), ratings=array([2., 1.])))\n",
    "    \"\"\"\n",
    "    # use this generator (https://numpy.org/doc/stable/reference/random/index.html)\n",
    "    # you should use rng.uniform() once inside this function to match the automatic test case\n",
    "    rng = np.random.default_rng(seed)\n",
    "\n",
    "    ####################################\n",
    "    ### ___ Enter your code here ___ ###\n",
    "    ####################################\n",
    "\n",
    "    return train_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(split_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data = split_dataset(pruned_dataset, p_test=0.1, seed=10)\n",
    "print(\"Number of training points:\", len(train_data.ratings))\n",
    "print(\"Number of test points:\", len(test_data.ratings))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Visualize the dataset\n",
    "So far, our data has been represented by a list of ratings. We will look at how we can interpret these ratings in a matrix (movies x users) and plot it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def to_matrix(dataset, num_movies, num_users):\n",
    "    \"\"\"\n",
    "    Construct a dense matrix out of the dataset.\n",
    "\n",
    "    Input:\n",
    "        dataset: Dataset\n",
    "\n",
    "    Output:\n",
    "        matrix: np.array of floats: (# movies, # users) -> rating (float) or np.NaN if unavailable\n",
    "\n",
    "    >>> to_matrix(Dataset(np.array([1, 1, 0]), np.array([0, 1, 0]), np.array([1.0, 3.0, 2.5])), 3, 2)\n",
    "    array([[2.5, nan],\n",
    "           [1. , 3. ],\n",
    "           [nan, nan]])\n",
    "    \"\"\"\n",
    "    m = (\n",
    "        np.zeros([num_movies, num_users]) * np.NaN\n",
    "    )  # We want NaNs for unavailable ratings\n",
    "    ####################################\n",
    "    ### ___ Enter your code here ___ ###\n",
    "    ####################################\n",
    "    ### HINT: Edit `m` by filling in the available ratings\n",
    "    return m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(to_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the train and test dataset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, axes = plt.subplots(ncols=3, figsize=(8, 3))\n",
    "for ax, label, data in zip(\n",
    "    axes, (\"All data\", \"Train data\", \"Test data\"), (dataset, train_data, test_data)\n",
    "):\n",
    "    im = ax.matshow(to_matrix(data, num_movies, num_users))\n",
    "    ax.set_title(label)\n",
    "    ax.set_xlabel(\"Movie\")\n",
    "    ax.set_ylabel(\"User\")\n",
    "fig.colorbar(im, label=\"Rating\");"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Comparing predictors\n",
    "\n",
    "The goal of recommender systems is to fill in those matrices. In the remainder of this Notebook, we will implement different methods of making those predictions. Let's first develop the tools to compare them.\n",
    "\n",
    "Our prediction methods will all have the following form:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Predictor:\n",
    "    def __init__(self, train_data: Dataset):\n",
    "        # This is where you can do any preparation or 'training' if necessary.\n",
    "        pass\n",
    "\n",
    "    def __call__(self, test_data: Dataset):\n",
    "        \"\"\"\n",
    "        Make predictions for the users and movies in the test dataset\n",
    "        (without looking at the ratings).\n",
    "\n",
    "        Inputs:\n",
    "            test_data: Dataset (with `n` entries)\n",
    "\n",
    "        Output:\n",
    "            predictions: np.array of floats, shape (`n`)\n",
    "        \"\"\"\n",
    "        raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " You would use a predictor like this as follows:\n",
    " \n",
    " ```\n",
    " p = Predictor(train_data)  # this initializes the model. This may include training.\n",
    " predictions = p(test_data)  # this calls `__call__` and makes a prediction.\n",
    " ```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As an example, a predictor that recommends the same rating everywhere would look like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GlobalMeanPredictor(Predictor):\n",
    "    def __init__(self, train_data: Dataset):\n",
    "        ####################################\n",
    "        ### ___ ENTER YOUR CODE HERE ___ ###\n",
    "        ####################################\n",
    "        self.mean = ...\n",
    "\n",
    "    def __call__(self, test_data: Dataset):\n",
    "        \"\"\"\n",
    "        >>> train_data = Dataset(np.array([1, 2, 3, 4]), np.array([1, 2, 3, 4]), np.array([1.0, 2.0, 2.0, 5.0]))\n",
    "        >>> test_data = Dataset(np.array([1, 2, 3, 4]), np.array([2, 1, 4, 0]), np.array([4.0, 1.0, 1.0, 2.0]))\n",
    "        >>> mean_predictor = GlobalMeanPredictor(train_data)\n",
    "        >>> mean_predictor(test_data)\n",
    "        array([2.5, 2.5, 2.5, 2.5])\n",
    "        \"\"\"\n",
    "        return np.full_like(test_data.ratings, fill_value=self.mean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(GlobalMeanPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can evaluate the predictions using Mean Squared Error:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(predictions, real_ratings):\n",
    "    \"\"\"Compute the mean squared prediction error\n",
    "\n",
    "    Inputs:\n",
    "        predictions: np.array of floats, shape (n)\n",
    "        real_ratings: np.array of floats, shape (n)\n",
    "\n",
    "    Returns:\n",
    "        mean squared error: float\n",
    "\n",
    "    >>> mse(np.array([1., 1.2, 2.]), np.array([2., 1., 1.5]))\n",
    "    0.43\n",
    "    >>> mse(np.array([4., 2., 1.]), np.array([4., 3., 1.]))\n",
    "    0.3333333333333333\n",
    "    \"\"\"\n",
    "    ####################################\n",
    "    ### ___ Enter your code here ___ ###\n",
    "    ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(mse)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The error for the global mean predictor can now be computed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_global_mean = GlobalMeanPredictor(train_data)\n",
    "\n",
    "mse(predict_global_mean(test_data), test_data.ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is always good to include simple baselines like this one in your machine learning experimentation. We now know that we should at least beat an MSE of `1.24` on the test set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Baselines "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will now gradually make the baseline predictor more complex, by differentiating between users. The idea is that some users will give higher scores on average than others."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User's mean prediction\n",
    "\n",
    "Here, to predict the score for a tuple `(movie, user)`, we will return the  average all movie ratings of the `user`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UserMeanPredictor(Predictor):\n",
    "\n",
    "    def __init__(self, train_data: Dataset):\n",
    "        ####################################\n",
    "        ### ___ Enter your code here ___ ###\n",
    "        ####################################\n",
    "        # Hint: don't worry about making this part fast.\n",
    "\n",
    "    def __call__(self, test_data: Dataset):\n",
    "        \"\"\"\n",
    "        Predict the mean rating of the user we are predicting for.\n",
    "\n",
    "        >>> train_data = Dataset(np.array([1, 2, 3, 4]), np.array([1, 2, 2, 0]), np.array([1.0, 2.0, 2.5, 5.0]))\n",
    "        >>> test_data = Dataset(np.array([1, 2, 3, 4]), np.array([2, 1, 1, 0]), np.array([4.0, 1.0, 1.0, 2.0]))\n",
    "        >>> mean_predictor = UserMeanPredictor(train_data)\n",
    "        >>> mean_predictor(test_data)\n",
    "        array([2.25, 1.  , 1.  , 5.  ])\n",
    "        \"\"\"\n",
    "        ####################################\n",
    "        ### ___ Enter your code here ___ ###\n",
    "        ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(UserMeanPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Is this working better than predicting the global mean?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_user_mean = UserMeanPredictor(train_data)\n",
    "\n",
    "mse(predict_user_mean(test_data), test_data.ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: Why would always predicting the user's mean be pretty useless in practice?\n",
    "\n",
    "Q: If you want, you can improve the predictor by falling back to the global mean in case we haven't seen any ratings of a user before (or not enough of them)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Movie's mean prediction\n",
    "\n",
    "Here, to predict the score for a tuple `(movie, user)`, we will return the  average all movie ratings of the `movie`. \n",
    "\n",
    "Under what assumption is this a good model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MovieMeanPredictor(Predictor):\n",
    "    def __init__(self, train_data: Dataset):\n",
    "        ####################################\n",
    "        ### ___ Enter your code here ___ ###\n",
    "        ####################################\n",
    "        # Hint: don't worry about making this part fast.\n",
    "\n",
    "    def __call__(self, test_data: Dataset):\n",
    "        \"\"\"\n",
    "        Predict the mean rating of the movie, irrespective of the user\n",
    "\n",
    "        >>> train_data = Dataset(np.array([0, 0, 1, 1, 2]), np.array([1, 2, 3, 4, 5]), np.array([4.0, 1.0, 1.0, 2.0, 1.0]))\n",
    "        >>> test_data = Dataset(np.array([0, 1, 2, 1]), np.array([1, 2, 2, 0]), np.array([1.0, 2.0, 2.5, 5.0]))\n",
    "        >>> mean_predictor = MovieMeanPredictor(train_data)\n",
    "        >>> mean_predictor(test_data)\n",
    "        array([2.5, 1.5, 1. , 1.5])\n",
    "        \"\"\"\n",
    "        ####################################\n",
    "        ### ___ Enter your code here ___ ###\n",
    "        ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(MovieMeanPredictor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_movie_mean = MovieMeanPredictor(train_data)\n",
    "\n",
    "mse(predict_movie_mean(test_data), test_data.ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Matrix Factorization\n",
    "\n",
    "With those baselines, we can implement a more realistic recommendation model that using Matrix Factorization.\n",
    "\n",
    "- To each movie $i$ in the dataset, we will assign a $d$-dimensional trainable ‘representation vector’ $\\mathbf{m}_i$.\n",
    "- To each user $j$ in the dataset, we will assign a $d$-dimensional trainable ‘representation vector’ $\\mathbf{u}_j$.\n",
    "- You can summarize those representations in two matrices: $\\mathbf{M}$ of size `(#movies, d)`, and $\\mathbf{U}$ of size `(#users, d)`.\n",
    "- The predicted score we will give for a movie $i$ and a user $j$ will be the dot product $\\mathbf{m}_i \\cdot \\mathbf{u}_j$.\n",
    "- You can simultaneously express the predicted scores for all movies and all users as a matrix of size `(#movies, #users)` as the matrix product $\\mathbf{M} \\mathbf{U}^\\top$. This interpretation gives the name to this model.\n",
    "- Given a training set $T$ with rating triples `(movie i, user j, rating r)`,\n",
    "  we will optimize the mean squared prediction error $\\frac{1}{|T|}\\sum_{(i, j, r) \\in T} (\\mathbf{m}_i \\cdot \\mathbf{u}_j - r)^2$ over the training set. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MatrixFactorizationPredictor(Predictor):\n",
    "\n",
    "    def __init__(self, train_data: Dataset, num_features=20, seed=1):\n",
    "        # Randomly initialize features for the users and the movies from N(0, 1)\n",
    "\n",
    "        # use this generator (https://numpy.org/doc/stable/reference/random/index.html)\n",
    "        # you are expected to use rng.normal() twice in this function to match the tests, once for movies, and then once for users\n",
    "        rng = np.random.default_rng(seed)\n",
    "\n",
    "        num_movies = np.max(train_data.movies) + 1\n",
    "        num_users = np.max(train_data.users) + 1\n",
    "        ####################################\n",
    "        ### ___ ENTER YOUR CODE HERE ___ ###\n",
    "        ####################################\n",
    "        self.movie_features = ...\n",
    "        self.user_features = ...\n",
    "\n",
    "        # Normally, you should train the model here, but we will skip this\n",
    "        # for now, to be able to take it step-by-step.\n",
    "\n",
    "    def __call__(self, test_data: Dataset):\n",
    "        \"\"\"\n",
    "        Predict the rating of a user/movie pair as the dot-product\n",
    "        of representation vectors of the user and the movie.\n",
    "\n",
    "        >>> train_data = Dataset(np.array([0, 0, 1, 1, 2]), np.array([1, 2, 3, 4, 5]), np.array([4.0, 1.0, 1.0, 2.0, 1.0]))\n",
    "        >>> test_data = Dataset(np.array([0, 1, 2, 1]), np.array([1, 2, 2, 0]), np.array([1.0, 2.0, 2.5, 5.0]))\n",
    "        >>> mean_predictor = MatrixFactorizationPredictor(train_data)\n",
    "        >>> mean_predictor(test_data)  # the factorization is not yet optimized here\n",
    "        array([ 2.62654714, -2.89866225,  0.70909287,  5.29901482])\n",
    "        \"\"\"\n",
    "        ####################################\n",
    "        ### ___ Enter your code here ___ ###\n",
    "        ####################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test(MatrixFactorizationPredictor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Without training, the `MatrixFactorizationPredictor` would not perform so well:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict_global_mean = MatrixFactorizationPredictor(train_data)\n",
    "\n",
    "mse(predict_global_mean(test_data), test_data.ratings)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize the factorization using SGD\n",
    "\n",
    "One way to optimize the representation vectors for movies and users, is to sample terms from the following loss function:\n",
    "\n",
    "$$\\frac{1}{|T|}\\sum_{(i, j, r) \\in T} (\\mathbf{m}_i \\cdot \\mathbf{u}_j - r)^2$$\n",
    "\n",
    "and optimize the sampled $\\mathbf{m}_i$ and $\\mathbf{u}_j$ using a stochastic gradient descent step.\n",
    "\n",
    "To improve generalization, we will add L2 regularization (also known as `weight decay`) to this model, both for the movie representations as well as for the user representations. Because those weights are unique for each movie / user, you should also sample those stochastically. If you sample a triple `(i, j, r)`, you should minimize:\n",
    "\n",
    "$$ (\\mathbf{m}_i \\cdot \\mathbf{u}_j - r)^2 + \\lambda_\\text{movie} \\left\\lVert\\mathbf{m}_i\\right\\rVert^2 + \\lambda_\\text{user} \\left\\lVert\\mathbf{u}_j\\right\\rVert^2$$\n",
    "\n",
    "You should start by computing a gradient for this term."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = 0.02\n",
    "weight_decay_movie = 0.3\n",
    "weight_decay_user = 0.3\n",
    "\n",
    "predictor = MatrixFactorizationPredictor(train_data)\n",
    "rng = np.random.default_rng(0)\n",
    "for epoch in range(20):\n",
    "    # Shuffle the dataset\n",
    "    datapoint_indices = rng.permutation(len(train_data.ratings))\n",
    "    for point in datapoint_indices:\n",
    "        movie = train_data.movies[point]\n",
    "        user = train_data.users[point]\n",
    "        rating = train_data.ratings[point]\n",
    "\n",
    "        # Optimize the factorizations in `predictor` using an SGD\n",
    "        # step based on the datapoint (movie, user, rating).\n",
    "        ####################################\n",
    "        ### ___ Enter your code here ___ ###\n",
    "        ####################################\n",
    "\n",
    "    learning_rate *= 0.9  # decay the learning rate after each epoch\n",
    "\n",
    "    print(\n",
    "        f\"Train error after epoch {epoch+1}: {mse(predictor(train_data), train_data.ratings)}\"\n",
    "    )\n",
    "    print(\n",
    "        f\"Test error after epoch {epoch+1}: {mse(predictor(test_data), test_data.ratings)}\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should expect the test error to go down below 1.0.\n",
    "\n",
    "Feel free to play with the weight decay parameters and learning rates to improve these results."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimize the factoriziation using Alternating Least Squares\n",
    "\n",
    "Instead of optimizing the objective\n",
    "\n",
    "$$\\frac{1}{|T|}\\sum_{(i, j, r) \\in T} (\\mathbf{m}_i \\cdot \\mathbf{u}_j - r)^2 + \\lambda_\\text{movie} \\left\\lVert \\mathbf{M} \\right\\rVert^2_\\text{F} + \\lambda_\\text{user} \\left\\lVert \\mathbf{U} \\right\\rVert^2_\\text{F}$$\n",
    "\n",
    "using SGD by sampling terms, we can also use ‘Alternating Least Squares’. For ALS, we make the observation that, if all users representations $\\mathbf{u}_j$ are fixed, optimizing all $\\mathbf{m}_i$ is a simple least squares problem. Similarly, if the movie representations $\\mathbf{m}_i$ are all fixed, optimizing all $\\mathbf{u}_j$ is a simple least squares problem. \n",
    "\n",
    "What we will do here, is alternate between the following steps:\n",
    "- Fix the user representations, and perfectly optimize the movie representations.\n",
    "- Fix the movie representations, and perfectly optimize the user representations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "movie_regularization = 20\n",
    "user_regularization = 20\n",
    "max_iterations = 1000\n",
    "stop_criterion = 1e-4\n",
    "\n",
    "predictor = MatrixFactorizationPredictor(train_data)\n",
    "user_features = predictor.user_features\n",
    "movie_features = predictor.movie_features\n",
    "rng = np.random.default_rng(0)\n",
    "\n",
    "prev_train_error = None\n",
    "\n",
    "for iteration in range(max_iterations):\n",
    "    # Optimize the user features\n",
    "    for user in np.unique(train_data.users):\n",
    "        # Update `user_features[user]` by optimizing the regularized corresponding least squares objective\n",
    "        mask = train_data.users == user\n",
    "        user_movies = train_data.movies[mask]\n",
    "        ####################################\n",
    "        ### ___ Enter your code here ___ ###\n",
    "        ####################################\n",
    "\n",
    "    # Optimize the movie features using least squares\n",
    "    for movie in np.unique(train_data.movies):\n",
    "        # Update `movie_features[movie]` by optimizing the regularized corresponding least squares objective\n",
    "        mask = train_data.movies == movie\n",
    "        movie_users = train_data.users[mask]\n",
    "        ####################################\n",
    "        ### ___ Enter your code here ___ ###\n",
    "        ####################################\n",
    "\n",
    "    train_error = mse(predictor(train_data), train_data.ratings)\n",
    "    print(f\"Train error after step {iteration+1}: {train_error}\")\n",
    "    print(\n",
    "        f\"Test error after step {iteration+1}: {mse(predictor(test_data), test_data.ratings)}\"\n",
    "    )\n",
    "\n",
    "    # Stop if the training error is not going down more than 'stop_criterion'\n",
    "    ####################################\n",
    "    ### ___ Enter your code here ___ ###\n",
    "    ####################################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should find that this beats the simple baselines.\n",
    "Using default SGD parameters, the alternating least squares solution here outperforms it, but by tuning SGD carefully, you should be able to get the same quality."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "365f77e409f1702921ebe0366eb00efc375f20dc8026da2eba9b2f7ebb727c11"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
